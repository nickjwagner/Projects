{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "69b9a648-bcc7-490d-9f9b-ea244d156bd6"
   },
   "source": [
    "# Web Scraping for Indeed.com & Predicting Salaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "34681254-c802-462f-829d-8894d0772d08"
   },
   "source": [
    "In this project, we will practice two major skills: collecting data by scraping a website and then building a binary classifier.\n",
    "\n",
    "We are going to collect salary information on data science jobs in a variety of markets. Then using the location, title and summary of the job we will attempt to predict the salary of the job. For job posting sites, this would be extraordinarily useful. While most listings DO NOT come with salary information (as you will see in this exercise), being to able extrapolate or predict the expected salaries from other listings can help guide negotiations.\n",
    "\n",
    "Normally, we could use regression for this task; however, we will convert this problem into classification and use a random forest regressor, as well as another classifier of your choice; either logistic regression, SVM, or KNN. \n",
    "\n",
    "- **Question**: Why would we want this to be a classification problem?\n",
    "- **Answer**: While more precision may be better, there is a fair amount of natural variance in job salaries - predicting a range be may be useful.\n",
    "\n",
    "Therefore, the first part of the assignment will be focused on scraping Indeed.com. In the second, we'll focus on using listings with salary information to build a model and predict additional salaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a948d79c-5527-4c0d-ab23-f5d43ce72056"
   },
   "source": [
    "### Scraping job listings from Indeed.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": true,
    "id": "7203e0c9-e437-4802-a6ad-7dc464f94436"
   },
   "source": [
    "We will be scraping job listings from Indeed.com using BeautifulSoup. Luckily, Indeed.com is a simple text page where we can easily find relevant entries.\n",
    "\n",
    "First, look at the source of an Indeed.com page: (http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l=New+York&start=10\")\n",
    "\n",
    "Notice, each job listing is underneath a `div` tag with a class name of `result`. We can use BeautifulSoup to extract those. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "9732c901-ae26-4160-8376-42e22dd327df"
   },
   "source": [
    "#### Setup a request (using `requests`) to the URL below. Use BeautifulSoup to parse the page and extract all results (HINT: Look for div tags with class name result)\n",
    "\n",
    "The URL here has many query parameters\n",
    "\n",
    "- `q` for the job search\n",
    "- This is followed by \"+20,000\" to return results with salaries (or expected salaries >$20,000)\n",
    "- `l` for a location \n",
    "- `start` for what result number to start on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "focus": false,
    "id": "2efefc73-064a-482d-b3b5-ddf5508cb4ec"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "bb0b866a-26a7-45e9-8084-5a0f90eb4b3e"
   },
   "source": [
    "Let's look at one result more closely. A single `result` looks like\n",
    "\n",
    "```\n",
    "<div class=\" row result\" data-jk=\"2480d203f7e97210\" data-tn-component=\"organicJob\" id=\"p_2480d203f7e97210\" itemscope=\"\" itemtype=\"http://schema.org/JobPosting\">\n",
    "<h2 class=\"jobtitle\" id=\"jl_2480d203f7e97210\">\n",
    "<a class=\"turnstileLink\" data-tn-element=\"jobTitle\" onmousedown=\"return rclk(this,jobmap[0],1);\" rel=\"nofollow\" target=\"_blank\" title=\"AVP/Quantitative Analyst\">AVP/Quantitative Analyst</a>\n",
    "</h2>\n",
    "<span class=\"company\" itemprop=\"hiringOrganization\" itemtype=\"http://schema.org/Organization\">\n",
    "<span itemprop=\"name\">\n",
    "<a href=\"/cmp/Alliancebernstein?from=SERP&amp;campaignid=serp-linkcompanyname&amp;fromjk=2480d203f7e97210&amp;jcid=b374f2a780e04789\" target=\"_blank\">\n",
    "    AllianceBernstein</a></span>\n",
    "</span>\n",
    "<tr>\n",
    "<td class=\"snip\">\n",
    "<nobr>$117,500 - $127,500 a year</nobr>\n",
    "<div>\n",
    "<span class=\"summary\" itemprop=\"description\">\n",
    "C onduct quantitative and statistical research as well as portfolio management for various investment portfolios. Collaborate with Quantitative Analysts and</span>\n",
    "</div>\n",
    "</div>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "</div>\n",
    "```\n",
    "\n",
    "While this has some more verbose elements removed, we can see that there is some structure to the above:\n",
    "- The salary is available in a `nobr` element inside of a `td` element with `class='snip`.\n",
    "- The title of a job is in a link with class set to `jobtitle` and a `data-tn-element=\"jobTitle`.  \n",
    "- The location is set in a `span` with `class='location'`. \n",
    "- The company is set in a `span` with `class='company'`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "f1eddb90-4ba8-483c-a229-77e93aa53119"
   },
   "source": [
    "### Write 4 functions to extract each item: location, company, job, and salary.\n",
    "\n",
    "example: \n",
    "```python\n",
    "def extract_location_from_result(result):\n",
    "    return result.find ...\n",
    "```\n",
    "\n",
    "\n",
    "- **Make sure these functions are robust and can handle cases where the data/field may not be available.**\n",
    "    - Remember to check if a field is empty or `None` for attempting to call methods on it\n",
    "    - Remember to use `try/except` if you anticipate errors\n",
    "- **Test** the functions on the results above and simple examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "34070e89-9521-4b45-90c8-57a6599aac68"
   },
   "source": [
    "Now, to scale up our scraping, we need to accumulate more results. We can do this by examining the URL above.\n",
    "\n",
    "- \"http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l=New+York&start=10\"\n",
    "\n",
    "There are two query parameters here we can alter to collect more results, the `l=New+York` and the `start=10`. The first controls the location of the results (so we can try a different city). The second controls where in the results to start and gives 10 results (thus, we can keep incrementing by 10 to go further in the list)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "e8beed7c-3e42-40c0-810f-5f67f8f885a0"
   },
   "source": [
    "#### Complete the following code to collect results from multiple cities and starting points. \n",
    "- Enter your city below to add it to the search\n",
    "- Remember to convert your salary to U.S. Dollars to match the other cities if the currency is different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_results_per_city = 4000 \n",
    "\n",
    "results = []\n",
    "\n",
    "for city in set(['New+York', 'Chicago', 'San+Francisco', 'Austin', 'Seattle', \n",
    "    'Los+Angeles', 'Philadelphia', 'Atlanta', 'Dallas', 'Pittsburgh', \n",
    "    'Portland', 'Phoenix', 'Denver', 'Houston', 'Miami', 'Washington+City%2C+DC']):\n",
    "    for start in range(0, max_results_per_city, 10):\n",
    "        r = requests.get('http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l=' + city + '&jt=fulltime&explvl=entry_level&start=' + str(start))\n",
    "        b = BeautifulSoup(r.text)\n",
    "        results.append(b.find_all(\"div\", class_ = \"result\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data in results is a list of lists. To make processing easier, I flatten to an unnested list.\n",
    "results = [item for sublist in results for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Crafting from \"results\" a dictionary (\"extracted\") of desired elements (job title, company, salary, location,\n",
    "and a short job summary).'''\n",
    "\n",
    "extracted = {}\n",
    "\n",
    "def extraction(results):\n",
    "    titles = []\n",
    "    companies = []\n",
    "    salaries = []\n",
    "    locations = []\n",
    "    summaries = []\n",
    "    for result in results:\n",
    "        \n",
    "        # job title\n",
    "        titles.append(result.find('a',{'data-tn-element':'jobTitle'}).text)\n",
    "        \n",
    "        # company\n",
    "        try:\n",
    "            companies.append(result.find(\"span\",{'class':'company'}).get_text(strip=True))\n",
    "        except AttributeError:\n",
    "            companies.append(None)\n",
    "            \n",
    "        # salary    \n",
    "        try:\n",
    "            salaries.append(result.find(name='nobr').text)\n",
    "        except AttributeError:\n",
    "            salaries.append(None)\n",
    "     \n",
    "        # location\n",
    "        locations.append(result.find(\"span\", class_ = \"location\").text)\n",
    "        \n",
    "        # summary\n",
    "        summaries.append(result.find(\"span\", class_ = \"summary\").get_text(strip=True))\n",
    "        \n",
    "    extracted['titles'] = titles\n",
    "    extracted['companies'] = companies\n",
    "    extracted['salaries'] = salaries\n",
    "    extracted['locations'] = locations\n",
    "    extracted['summaries'] = summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "listings = pd.DataFrame.from_dict(extracted)\n",
    "\n",
    "# I pulled over 90,000 job postings from Indeed!\n",
    "listings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Since I'm working with a big file that takes > an hour to scrape, best to save the scraped data on my computer \n",
    "(to avoid having to rescrape if I have issues with my kernel).'''\n",
    "\n",
    "listings.to_csv('/users/nick/desktop/indeed_jobs.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "listings = pd.read_csv('/users/nick/desktop/indeed_jobs.csv')\n",
    "listings.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "20339c09-5032-4e27-91be-286e9b46cd13"
   },
   "source": [
    "#### Use the functions you wrote above to parse out the 4 fields - location, title, company and salary. Create a dataframe from the results with those 4 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Unfortunately, most of the job postings did not include salary information. Since these rows won't be any good for \n",
    "model-building, I'll drop them.'''\n",
    "\n",
    "print listings[listings.salaries.isnull()].shape\n",
    "listings.dropna(subset=['salaries'], how='all', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Getting rid of non-annual (hourly, weekly, monthly) salaries.\n",
    "listings = listings[~listings.salaries.str.contains('month') & ~listings.salaries.str.contains('week') & ~listings.salaries.str.contains('hour')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Another issue to be wary of: a single job that was posted multiple times. How many are there? (I want them dropped.)\n",
    "print listings.duplicated().sum()\n",
    "\n",
    "# Out of curiosity -- which jobs are reposted?\n",
    "listings.loc[listings.duplicated(keep=False), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dropping duplicates\n",
    "listings.drop_duplicates(inplace=True)\n",
    "\n",
    "# Necessary for iterating via 'enumeration' (see below).\n",
    "listings.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "ff98ce64-78a7-441f-a675-63464e32c834"
   },
   "source": [
    "Lastly, we need to clean up salary data. \n",
    "\n",
    "1. Only a small number of the scraped results have salary information - only these will be used for modeling.\n",
    "1. Some of the salaries are not yearly but hourly or weekly, these will not be useful to us for now\n",
    "1. Some of the entries may be duplicated\n",
    "1. The salaries are given as text and usually with ranges.\n",
    "\n",
    "#### Find the entries with annual salary entries, by filtering the entries without salaries or salaries that are not yearly (filter those that refer to hour or week). Also, remove duplicate entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Getting rid of commas and $s in the salaries column.\n",
    "listings.salaries = listings.salaries.apply(lambda x: x.replace(',',''))\n",
    "listings.salaries = listings.salaries.apply(lambda x: x.replace('$',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "''' Creating a list separate from my dataframe only to insert it into my dataframe may be an unnecessary step,\n",
    "(instead of making a list, why not make a new column -- or better, overwrite the source column?), but it's \n",
    "easy enough and I know how to do it. '''\n",
    "salaries = []\n",
    "for salary in listings.salaries:\n",
    "    salaries.append(re.findall(r'\\d+', salary))\n",
    "    \n",
    "listings['salary'] = salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "''' Checking data type of entries in 'salary' column. If I don't include the inside for loop, the type of each \n",
    "cell would be 'list.' ''' \n",
    "for i, e in enumerate(listings.salary):\n",
    "    for a, b in enumerate(listings.salary[i]):\n",
    "        print type(listings.salary[i][a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To calculate median salary, I'll need to convert \"str\" to \"int.\"\n",
    "for i,e in enumerate(listings.salary):\n",
    "    for a,b in enumerate(listings.salary[i]):\n",
    "        listings.salary[i][a] = int(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Every value, whether a pair of values or a single value, is within a list. But by performing an operation on the \n",
    "values inside the list a single (non-list) value is returned.'''\n",
    "for i,e in enumerate(listings.salary):\n",
    "    if len(e) > 1:\n",
    "        listings.salary[i] = sum(e)/2.\n",
    "    else:\n",
    "        listings.salary[i] = sum(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 'salaries', which provides the text from which 'salary' was derived, won't be used in modeling and can be dropped.\n",
    "listings.drop('salaries', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Since the format is \"City, STATE\", this returns city. \n",
    "listings['city'] = listings.locations.apply(lambda x: x.split(',')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "c7631f51-07f2-4c79-a093-3e9bc7849a48"
   },
   "source": [
    "#### We want to predict a binary variable - whether the salary was low or high. Compute the median salary and create a new binary variable that is true when the salary is high (above the median)\n",
    "\n",
    "We could also perform Linear Regression (or any regression) to predict the salary value here. Instead, we are going to convert this into a _binary_ classification problem, by predicting two classes, HIGH vs LOW salary.\n",
    "\n",
    "While performing regression may be better, performing classification may help remove some of the noise of the extreme salaries. We don't have to choice the `median` as the splitting point - we could also split on the 75th percentile or any other reasonable breaking point.\n",
    "\n",
    "In fact, the ideal scenario may be to predict many levels of salaries, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set a default value for whether or not a given job's salary is above the median.\n",
    "listings['above_median'] = 0\n",
    "\n",
    "for i, e in enumerate(listings.above_median):\n",
    "    if listings.salary[i] > listings.salary.median():\n",
    "        listings.above_median[i] = 1\n",
    "    else:\n",
    "        listings.above_median[i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "04563b69-f7b6-466f-9d65-fc62c9ddee6a"
   },
   "source": [
    "## Predicting salaries using Random Forests + Another Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest - Get Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "588f9845-6143-4bcc-bfd1-85d45b79303d"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = listings.city\n",
    "y = listings.above_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X)\n",
    "# Since New York has the largest population, I'll drop it -- it acts as 'baseline' for the rest of my dummy population.\n",
    "X.drop('New York', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = KFold(len(y), n_folds=3, shuffle=True, random_state=41)\n",
    "rfc = RandomForestClassifier()\n",
    "cross_val_score(rfc, X, y, cv = cv).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "param_grid = {'n_estimators':[8,10,12,20],\n",
    "              'criterion': ['gini', 'entropy'],\n",
    "              'max_features':['sqrt', 'log2', None],\n",
    "              'min_samples_split':[2,4,6,8],\n",
    "              'min_samples_leaf': [1,2,3,4]}\n",
    "\n",
    "gs = GridSearchCV(rfc, param_grid, cv=cv).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print gs.best_score_\n",
    "print gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Random Forest - LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "''' I'm using labelencoder mainly so that I can plot. (Plotting not viable when city is split over 30+ dummy variables.) \n",
    "But it also makes a leaner dataframe, and since I'm using randomforest, it should have no impact on outcome.'''\n",
    "\n",
    "X = listings.city\n",
    "y = listings.above_median\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "X = pd.DataFrame(LabelEncoder().fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.scatter(X, listings.salary, c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = KFold(len(y), n_folds=3, shuffle=True, random_state=41)\n",
    "rfc = RandomForestClassifier()\n",
    "cross_val_score(rfc, X, y, cv = cv).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators':[8,10,12,20],\n",
    "              'criterion': ['gini', 'entropy'],\n",
    "              'max_features':['sqrt', 'log2', None],\n",
    "              'min_samples_split':[2,4,6,8],\n",
    "              'min_samples_leaf': [1,2,3,4]}\n",
    "\n",
    "gs = GridSearchCV(rfc, param_grid, cv=cv).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print gs.best_score_\n",
    "print gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I have too many classes, with too few observations, to run a logistic regression. That's okay. It would've \n",
    "been a pretty hokey solution anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "0ef04f32-419c-4bf2-baf7-48201f03df89"
   },
   "source": [
    "#### Create a few new variables in your dataframe to represent interesting features of a job title.\n",
    "- For example, create a feature that represents whether 'Senior' is in the title \n",
    "- or whether 'Manager' is in the title. \n",
    "- Then build a new Random Forest with these features. Do they add any value? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "listings.titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create two columns. One for jobs with senior keywords, the other for jobs with junior keywords. Set default to 0.\n",
    "listings['senior'] = 0\n",
    "listings['entry'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, e in enumerate(listings.senior):\n",
    "    if 'Lead' in listings.titles[i]:\n",
    "        listings.senior[i] = 1\n",
    "    elif 'Senior' in listings.titles[i]:\n",
    "        listings.senior[i] = 1\n",
    "    elif 'Manager' in listings.titles[i]:\n",
    "        listings.senior[i] = 1\n",
    "    elif 'Experienced' in listings.titles[i]:\n",
    "        listings.senior[i] = 1\n",
    "    else:\n",
    "        listings.senior[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, e in enumerate(listings.entry):\n",
    "    if 'Junior' in listings.titles[i]:\n",
    "        listings.entry[i] = 1\n",
    "    elif 'Entry' in listings.titles[i]:\n",
    "        listings.entry[i] = 1\n",
    "    else:\n",
    "        listings.entry[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# There weren't many jobs that could easily be identified as entry-level or senior-level.\n",
    "print listings.senior.sum()\n",
    "print listings.entry.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(listings.senior, listings.salary, c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(listings.entry, listings.salary, c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = listings.city\n",
    "X = pd.get_dummies(X)\n",
    "X.drop('New York', axis=1, inplace=True)\n",
    "X['senior'] = listings.senior\n",
    "X['entry'] = listings.entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(y, n_folds=5, shuffle=True, random_state=41)\n",
    "rfc = RandomForestClassifier(n_estimators = 20)\n",
    "cross_val_score(rfc, X, y, cv = cv).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(rfc, param_grid, cv=cv).fit(X, y)\n",
    "print gs.best_score_\n",
    "print gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# Laughably bad score!\n",
    "svc = svm.SVC(kernel = 'linear')\n",
    "cross_val_score(svc, X, y, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Even worse!! While gridsearching would make things better, this isn't a route worth pursuing.\n",
    "# Measuring vectors in an entirely categorical space is misguided.\n",
    "svc = svm.SVC(kernel = 'rbf')\n",
    "cross_val_score(svc, X, y, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF w/ Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = listings.loc[:, ['titles', 'summaries']]\n",
    "y = listings.above_median\n",
    "\n",
    "X['alltext'] = X.titles + X.summaries\n",
    "X.drop(['titles', 'summaries'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I don't want to deal with a list of lists, so I use ravel.\n",
    "X_train.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tvec = TfidfVectorizer()\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tvec.fit(X_train.values.ravel())\n",
    "\n",
    "X_train = pd.DataFrame(tvec.transform(X_train.values.ravel()).todense(),\n",
    "                   columns=tvec.get_feature_names())\n",
    "X_test = pd.DataFrame(tvec.transform(X_test.values.ravel()).todense(),\n",
    "                   columns=tvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)\n",
    "metrics.accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "ddbc6159-6854-4ca7-857f-bfecdaf6d9c2"
   },
   "source": [
    "## TF-IDF w/ SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = listings.loc[:, ['titles', 'summaries']]\n",
    "y = listings.above_median\n",
    "X['alltext'] = X.titles + X.summaries\n",
    "X.drop(['titles', 'summaries'], axis=1, inplace=True)\n",
    "X_train.values.ravel()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tvec = TfidfVectorizer()\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "tvec.fit(X_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(tvec.transform(X_train.values.ravel()).todense(),\n",
    "                   columns=tvec.get_feature_names())\n",
    "X_test = pd.DataFrame(tvec.transform(X_test.values.ravel()).todense(),\n",
    "                   columns=tvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "svc = svm.SVC()\n",
    "\n",
    "gamma_range = 10.**np.arange(-5, 2)\n",
    "C_range = 10.**np.arange(-2, 3)\n",
    "kernel_range = ['rbf', 'sigmoid', 'linear', 'poly']\n",
    "param_grid = dict(gamma=gamma_range, C=C_range, kernel=kernel_range)\n",
    "gs = GridSearchCV(svc, param_grid, cv=10, scoring='accuracy')\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print gs.best_score_\n",
    "print gs.best_params_\n",
    "print gs.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF w/ Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = listings.loc[:, ['titles', 'summaries']]\n",
    "y = listings.above_median\n",
    "X['alltext'] = X.titles + X.summaries\n",
    "X.drop(['titles', 'summaries'], axis=1, inplace=True)\n",
    "X_train.values.ravel()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tvec = TfidfVectorizer()\n",
    "tvec.fit(X.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(tvec.transform(X.values.ravel()).todense(),\n",
    "                   columns=tvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = KFold(len(y), n_folds=3, shuffle=True, random_state=41)\n",
    "rfc = RandomForestClassifier()\n",
    "cross_val_score(rfc, X, y, cv = cv).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators':[8,10,12,20],\n",
    "              'criterion': ['gini', 'entropy'],\n",
    "              'max_features':['sqrt', 'log2', None],\n",
    "              'min_samples_split':[2,4,6,8],\n",
    "              'min_samples_leaf': [1,2,3,4]}\n",
    "\n",
    "gs = GridSearchCV(rfc, param_grid, cv=cv).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print gs.best_score_\n",
    "print gs.best_params_\n",
    "print gs.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same as above, but with city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X['city'] = listings.city\n",
    "X.city = pd.DataFrame(LabelEncoder().fit_transform(X.city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs.fit(X, y)\n",
    "\n",
    "# Unfortunately, just a little bit worse.\n",
    "print gs.best_score_\n",
    "print gs.best_params_\n",
    "print gs.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = listings.loc[:, ['titles', 'summaries']]\n",
    "y = listings.above_median\n",
    "X['alltext'] = X.titles + X.summaries\n",
    "X.drop(['titles', 'summaries'], axis=1, inplace=True)\n",
    "X_train.values.ravel()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tvec = TfidfVectorizer()\n",
    "tvec.fit(X.values.ravel(), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(tvec.transform(X.values.ravel()).todense(),\n",
    "                   columns=tvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "cv = KFold(len(y), n_folds=3, shuffle=True, random_state=41)\n",
    "etc = ExtraTreesClassifier()\n",
    "cross_val_score(etc, X, y, cv = cv).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators':[8,10,12,20],\n",
    "              'criterion': ['gini', 'entropy'],\n",
    "              'max_features':['sqrt', 'log2', None],\n",
    "              'min_samples_split':[2,4,6,8],\n",
    "              'min_samples_leaf': [1,2,3,4]}\n",
    "\n",
    "gs = GridSearchCV(etc, param_grid, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs.fit(X, y)\n",
    "\n",
    "print gs.best_score_\n",
    "print gs.best_params_\n",
    "print gs.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor\n",
    "\n",
    "Let's try treating this as a regression problem. \n",
    "\n",
    "- Train a random forest regressor on the regression problem and predict your dependent.\n",
    "- Evaluate the score with a 5-fold cross-validation\n",
    "- Do a scatter plot of the predicted vs actual scores for each of the 5 folds, do they match?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = listings.city\n",
    "X = pd.get_dummies(X)\n",
    "X.drop('New York', axis=1, inplace=True)\n",
    "X['senior'] = listings.senior\n",
    "X['entry'] = listings.entry\n",
    "\n",
    "y = listings.salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfr = RandomForestRegressor()\n",
    "cross_val_score(rfr, X, y, cv = cv).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators':[8,10,12,20],\n",
    "              'max_features':['sqrt', 'log2', None],\n",
    "              'min_samples_split':[2,4,6,8],\n",
    "              'min_samples_leaf': [1,2,3,4]}\n",
    "\n",
    "gs = GridSearchCV(rfr, param_grid, cv=cv).fit(X, y)\n",
    "print gs.best_score_\n",
    "print gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(gs.predict(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression, but with TD-IDF'd X (plus location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = listings.loc[:, ['titles', 'summaries']]\n",
    "y = listings.above_median\n",
    "X['alltext'] = X.titles + X.summaries\n",
    "X.drop(['titles', 'summaries'], axis=1, inplace=True)\n",
    "X_train.values.ravel()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tvec = TfidfVectorizer()\n",
    "tvec.fit(X.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(tvec.transform(X.values.ravel()).todense(),\n",
    "                   columns=tvec.get_feature_names())\n",
    "X['city'] = listings.city\n",
    "X.city = pd.DataFrame(LabelEncoder().fit_transform(X.city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators':[8,10,12,20],\n",
    "              'max_features':['sqrt', 'log2', None],\n",
    "              'min_samples_split':[2,4,6,8],\n",
    "              'min_samples_leaf': [1,2,3,4]}\n",
    "\n",
    "gs = GridSearchCV(rfr, param_grid, cv=cv).fit(X, listings.salary)\n",
    "print gs.best_score_\n",
    "print gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(gs.predict(X), listings.salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = listings.loc[:, ['titles', 'summaries']]\n",
    "y = listings.above_median\n",
    "X['alltext'] = X.titles + X.summaries\n",
    "X.drop(['titles', 'summaries'], axis=1, inplace=True)\n",
    "X_train.values.ravel()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tvec = TfidfVectorizer()\n",
    "tvec.fit(X.values.ravel())\n",
    "X = pd.DataFrame(tvec.transform(X.values.ravel()).todense(),\n",
    "                   columns=tvec.get_feature_names())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

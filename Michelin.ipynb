{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Your goal is to use data science to produce a list of which restaurants you believe will be on the DC Michelin Guide, and how many stars each of your submitted restaurants earned. You are welcome to use any tools that you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import unicodedata\n",
    "import requests\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using yelp's API, collect data from Chicago, New York, San Francisco (all Michelin reviewed cities) & DC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "authorization = {'Authorization': ***** }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dc_yelp = []\n",
    "\n",
    "for offset in range(0, 300, 20):\n",
    "    try:\n",
    "        response = requests.get('https://api.yelp.com/v3/businesses/search?term=restaurants&location=Washington,+DC&offset={}&sort_by=rating&price=2,3,4'.format(offset), headers=authorization)\n",
    "        dc_yelp.append(json.loads(response.text))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chicago_yelp = []\n",
    "\n",
    "for offset in range(0, 300, 20):\n",
    "    try:\n",
    "        response = requests.get('https://api.yelp.com/v3/businesses/search?term=restaurants&location=Chicago&offset={}&sort_by=rating&price=2,3,4'.format(offset), headers=authorization)\n",
    "        chicago_yelp.append(json.loads(response.text))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ny_yelp = []\n",
    "\n",
    "for offset in range(0, 300, 20):\n",
    "    try:\n",
    "        response = requests.get('https://api.yelp.com/v3/businesses/search?term=restaurants&location=New+York&offset={}&sort_by=rating&price=2,3,4'.format(offset), headers=authorization)\n",
    "        ny_yelp.append(json.loads(response.text))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sf_yelp = []\n",
    "\n",
    "for offset in range(0, 300, 20):\n",
    "    try:\n",
    "        response = requests.get('https://api.yelp.com/v3/businesses/search?term=restaurants&location=San+Francisco&offset={}&sort_by=rating&price=2,3,4'.format(offset), headers=authorization)\n",
    "        sf_yelp.append(json.loads(response.text))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data into dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def df_maker(yelp):\n",
    "    rest_id = []\n",
    "    name = []\n",
    "    rating = []\n",
    "    review_count = []\n",
    "    price = []\n",
    "    categories = []\n",
    "    for dict in yelp:\n",
    "        for k, v in dict.iteritems():\n",
    "            if k == 'businesses':\n",
    "                for dict in v:\n",
    "                    for inner_k, inner_v in dict.iteritems():\n",
    "                        if inner_k == 'id':\n",
    "                            rest_id.append(inner_v)\n",
    "                        elif inner_k == 'name':\n",
    "                            name.append(inner_v)\n",
    "                        elif inner_k == 'rating':\n",
    "                            rating.append(inner_v)\n",
    "                        elif inner_k == 'review_count':\n",
    "                            review_count.append(inner_v)\n",
    "                        elif inner_k == 'price':\n",
    "                            price.append(inner_v)\n",
    "                        elif inner_k == 'categories':\n",
    "                            aliases = []\n",
    "                            for dict in inner_v:\n",
    "                                for deep_k, deep_v in dict.iteritems():\n",
    "                                    if deep_k == 'alias':\n",
    "                                        aliases.append(deep_v)\n",
    "                            categories.append(\" \".join(aliases))\n",
    "                                        \n",
    "                            \n",
    "    df = pd.DataFrame({ 'num_reviews': review_count, \n",
    "                        'price': price, \n",
    "                        'rating': rating,\n",
    "                        'restaurant': name,\n",
    "                        'rest_id': rest_id,\n",
    "                        'category': categories })\n",
    "\n",
    "    df = df[['restaurant', 'rest_id', 'rating', 'price', 'num_reviews', 'category']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dc = df_maker(dc_yelp)\n",
    "chi = df_maker(chicago_yelp)\n",
    "sf = df_maker(sf_yelp)\n",
    "ny = df_maker(ny_yelp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dc.restaurant = dc.restaurant.apply(lambda x: unicodedata.normalize('NFKD', x).encode('ascii', 'ignore'))\n",
    "sf.restaurant = sf.restaurant.apply(lambda x: unicodedata.normalize('NFKD', x).encode('ascii', 'ignore'))\n",
    "ny.restaurant = ny.restaurant.apply(lambda x: unicodedata.normalize('NFKD', x).encode('ascii', 'ignore'))\n",
    "chi.restaurant = chi.restaurant.apply(lambda x: unicodedata.normalize('NFKD', x).encode('ascii', 'ignore'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I prepared (manually) a list of all US Michelin starred restaurants. Since only some were picked on the first   sweep of yelp's API,  I'll need to do a second request, this time asking for specific restaurant names. If this creates duplicates (it will), I'll deal with them later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stars = pd.read_csv('/users/nick/desktop/michelin_stars.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf_stars = []\n",
    "for i in stars.loc[stars.SanFran.notnull()].SanFran:\n",
    "    sf_stars.append(i)\n",
    "    \n",
    "ny_stars = []\n",
    "for i in stars.loc[stars.NewYork.notnull()].NewYork:\n",
    "    ny_stars.append(i)\n",
    "    \n",
    "chi_stars = []\n",
    "for i in stars.loc[stars.Chicago.notnull()].Chicago:\n",
    "    chi_stars.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf_michelin = []\n",
    "\n",
    "for i in stars.loc[stars.SanFran.notnull()].SanFran:\n",
    "    response = requests.get('https://api.yelp.com/v3/businesses/search?term={}&location=San+Francisco&categories=restaurants'.format(i.replace(' ', '+')), headers=authorization)\n",
    "    try:\n",
    "        for r in range(response.json()['total']):\n",
    "            if (response.json()['businesses'][r]['name'] in i.decode('utf-8') or i.decode('utf-8') in response.json()['businesses'][r]['name']):\n",
    "                sf_michelin.append([response.json()['businesses'][r]])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ny_michelin = []\n",
    "\n",
    "for i in stars.loc[stars.NewYork.notnull()].NewYork:\n",
    "    response = requests.get('https://api.yelp.com/v3/businesses/search?term={}&location=New+York&categories=restaurants'.format(i.replace(' ', '+')), headers=authorization)\n",
    "    try:\n",
    "        for r in range(response.json()['total']):\n",
    "            if (response.json()['businesses'][r]['name'] in i.decode('utf-8') or i.decode('utf-8') in response.json()['businesses'][r]['name']):\n",
    "                if response.json()['businesses'][r]['review_count'] > 15:\n",
    "                    ny_michelin.append([response.json()['businesses'][r]])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chi_michelin = []\n",
    "\n",
    "for i in stars.loc[stars.Chicago.notnull()].Chicago:\n",
    "    response = requests.get('https://api.yelp.com/v3/businesses/search?term={}&location=Chicago&categories=restaurants'.format(i.replace(' ', '+')), headers=authorization)\n",
    "    try:\n",
    "        for r in range(response.json()['total']):\n",
    "            if (response.json()['businesses'][r]['name'] in i.decode('utf-8') or i.decode('utf-8') in response.json()['businesses'][r]['name']):\n",
    "                chi_michelin.append([response.json()['businesses'][r]])\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def df_maker_v2(reviews):\n",
    "    rest_id = []\n",
    "    name = []\n",
    "    rating = []\n",
    "    review_count = []\n",
    "    price = []\n",
    "    categories = []\n",
    "    for list in reviews:\n",
    "        for dict in list:\n",
    "            for k, v in dict.iteritems():\n",
    "                if k == 'id':\n",
    "                    rest_id.append(v)\n",
    "                elif k == 'name':\n",
    "                    name.append(v)\n",
    "                elif k == 'rating':\n",
    "                    rating.append(v)\n",
    "                elif k == 'review_count':\n",
    "                    review_count.append(v)\n",
    "                elif k == 'price':\n",
    "                    price.append(v)\n",
    "                elif k == 'categories':\n",
    "                            aliases = []\n",
    "                            for dict in v:\n",
    "                                for inner_k, inner_v in dict.iteritems():\n",
    "                                    if inner_k == 'alias':\n",
    "                                        aliases.append(inner_v)\n",
    "                            categories.append(\" \".join(aliases))\n",
    "                            \n",
    "    df = pd.DataFrame({ 'num_reviews': review_count, \n",
    "                        'price': price, \n",
    "                        'rating': rating,\n",
    "                        'restaurant': name,\n",
    "                        'rest_id': rest_id,\n",
    "                        'category': categories })\n",
    "\n",
    "    df = df[['restaurant', 'rest_id', 'rating', 'price', 'num_reviews', 'category']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf_stars = df_maker_v2(sf_michelin)\n",
    "ny_stars = df_maker_v2(ny_michelin)\n",
    "chi_stars = df_maker_v2(chi_michelin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updating dataframe with known Michelin star counts (from manually created list of Michelin starred restaurants)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for a, b in enumerate(sf_stars.restaurant):\n",
    "    try:\n",
    "        for x, y in enumerate(stars.loc[stars.SanFran.notnull()].SanFran):\n",
    "            if (b.encode('utf-8') in y or y in b.encode('utf-8')):\n",
    "                sf_stars.loc[a, 'stars'] = stars.loc[x, 'sf_stars']\n",
    "    except:\n",
    "        sf_stars.loc[a, 'stars'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for a, b in enumerate(ny_stars.restaurant):\n",
    "    try:\n",
    "        for x, y in enumerate(stars.loc[stars.NewYork.notnull()].NewYork):\n",
    "            if (b.encode('utf-8') in y or y in b.encode('utf-8')):\n",
    "                ny_stars.loc[a, 'stars'] = stars.loc[x, 'ny_stars']\n",
    "    except:\n",
    "        ny_stars.loc[a, 'stars'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for a, b in enumerate(chi_stars.restaurant):\n",
    "    try:\n",
    "        for x, y in enumerate(stars.loc[stars.Chicago.notnull()].Chicago):\n",
    "            if (b.encode('utf-8') in y or y in b.encode('utf-8')):\n",
    "                chi_stars.loc[a, 'stars'] = stars.loc[x, 'chi_stars']\n",
    "    except:\n",
    "        chi_stars.loc[a, 'stars'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ny = pd.concat([ny, ny_stars])\n",
    "ny.reset_index(drop=True, inplace=True)\n",
    "ny.stars.replace(np.nan, 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chi = pd.concat([chi, chi_stars])\n",
    "chi.reset_index(drop=True, inplace=True)\n",
    "chi.stars.replace(np.nan, 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sf = pd.concat([sf, sf_stars])\n",
    "sf.reset_index(drop=True, inplace=True)\n",
    "sf.stars.replace(np.nan, 0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DC touch-ups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Looking through the DC restaurants pulled from yelp, many were missing that I think will be Michelin contenders. \n",
    "So, as above, I'll return to yelp's API.''' \n",
    "contenders = [\"Kinship\", \"Marcel's\", \"Masseria\", \"Riggsby\", \"Garrison\", \"Mintwood\", \"Red Hen\", \"G by Mike Isabella\", \"Fiola Mare\", \"Le Diplomate\", \"Boss Shepherd's\", \"minibar\", \"The Dabney\", \"Metier\", \"Little Serow\", \"Source\", \"Obelisk\", \"Plume\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dc_contenders = []\n",
    "\n",
    "for i in contenders:\n",
    "    response = requests.get('https://api.yelp.com/v3/businesses/search?term={}&location=Washington,+DC&categories=restaurants'.format(i.replace(' ', '+')), headers=authorization)\n",
    "    try:\n",
    "        for r in range(response.json()['total']):\n",
    "            if (response.json()['businesses'][r]['name'] in i.decode('utf-8') or i.decode('utf-8') in response.json()['businesses'][r]['name']):\n",
    "                dc_contenders.append([response.json()['businesses'][r]])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dc_contenders = df_maker_v2(dc_contenders)\n",
    "dc = pd.concat([dc, dc_contenders])\n",
    "dc.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Because I made two passes through yelp's API, there are duplicates. Need to get rid of those. Also, some restaurants have multiple yelp pages. I want only those with the highest number of reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def duplicate_dropper(city):\n",
    "    \n",
    "    for i, e in enumerate(city.restaurant):\n",
    "        if e in city.loc[city.duplicated(subset='restaurant') & (city.stars > 0)].restaurant.values:\n",
    "            if city.loc[i, 'stars'] == 0:\n",
    "                city.drop(i, axis=0, inplace=True) \n",
    "            \n",
    "    city.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "    for i, e in enumerate(city.restaurant):\n",
    "        for r in range(len(city)):\n",
    "            try:\n",
    "                if (e == city.loc[r, 'restaurant'] and i != r):\n",
    "                    if city.loc[i, 'num_reviews'] > city.loc[r, 'num_reviews']:\n",
    "                        city.drop(r, axis=0, inplace=True)\n",
    "                    else:\n",
    "                        city.drop(i, axis=0, inplace=True)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    city.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "duplicate_dropper(chi)\n",
    "duplicate_dropper(ny)\n",
    "duplicate_dropper(sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Since 'dc' doesn't have a 'stars' columns (a column that's part of my duplicate checker function), I'll check\n",
    "for duplicates w/o using that function.'''\n",
    "\n",
    "for i, e in enumerate(dc.restaurant):\n",
    "    for r in range(len(dc)):\n",
    "        try:\n",
    "            if (e == dc.loc[r, 'restaurant'] and i != r):\n",
    "                if dc.loc[i, 'num_reviews'] > dc.loc[r, 'num_reviews']:\n",
    "                    dc.drop(r, axis=0, inplace=True)\n",
    "                else:\n",
    "                    dc.drop(i, axis=0, inplace=True)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "dc.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  I want review text, which requires another (tailored) API request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ny_reviews = []\n",
    "\n",
    "for i in ny.rest_id:\n",
    "    response = requests.get('https://api.yelp.com/v3/businesses/{}/reviews'.format(i.encode('utf-8')), headers=authorization)\n",
    "    store_reviews = []\n",
    "    for r in range(response.json()['total']):\n",
    "            store_reviews.append(response.json()['reviews'][r]['text'])\n",
    "    ny_reviews.append(\" \".join(store_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sf_reviews = []\n",
    "\n",
    "for i in sf.rest_id:\n",
    "    response = requests.get('https://api.yelp.com/v3/businesses/{}/reviews'.format(i.encode('utf-8')), headers=authorization)\n",
    "    store_reviews = []\n",
    "    for r in range(response.json()['total']):\n",
    "            store_reviews.append(response.json()['reviews'][r]['text'])\n",
    "    sf_reviews.append(\" \".join(store_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chi_reviews = []\n",
    "\n",
    "for i in chi.rest_id:\n",
    "    response = requests.get('https://api.yelp.com/v3/businesses/{}/reviews'.format(i.encode('utf-8')), headers=authorization)\n",
    "    store_reviews = []\n",
    "    for r in range(response.json()['total']):\n",
    "            store_reviews.append(response.json()['reviews'][r]['text'])\n",
    "    chi_reviews.append(\" \".join(store_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dc_reviews = []\n",
    "\n",
    "for i in dc.rest_id:\n",
    "    response = requests.get('https://api.yelp.com/v3/businesses/{}/reviews'.format(i.encode('utf-8')), headers=authorization)\n",
    "    store_reviews = []\n",
    "    for r in range(response.json()['total']):\n",
    "            store_reviews.append(response.json()['reviews'][r]['text'])\n",
    "    dc_reviews.append(\" \".join(store_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chi['text'] = chi.category + ' ' + chi_reviews\n",
    "ny['text'] = ny.category + ' ' + ny_reviews\n",
    "sf['text'] = sf.category + ' ' + sf_reviews\n",
    "dc['text'] = dc.category + ' ' + dc_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Change dollar signs to integers so price can be used for modeling.\n",
    "\n",
    "price_map = { '$$': 1,\n",
    "              '$$$': 2,\n",
    "              '$$$$': 3 }\n",
    "\n",
    "chi.price = chi.price.map(price_map)\n",
    "sf.price = sf.price.map(price_map)\n",
    "dc.price = dc.price.map(price_map)\n",
    "ny.price = ny.price.map(price_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DONE WITH DATA CLEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I will be modeling on San Francisco and New York data, and testing against Chicago data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some light text analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tvec = TfidfVectorizer(ngram_range=(1,2), stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf_ny = pd.concat([sf, ny])\n",
    "sf_ny.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tvec.fit(sf_ny.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_train = pd.DataFrame(tvec.transform(sf_ny.text).todense(),\n",
    "                   columns=tvec.get_feature_names())\n",
    "text_test = pd.DataFrame(tvec.transform(chi.text).todense(),\n",
    "                   columns=tvec.get_feature_names())\n",
    "\n",
    "y_train = sf_ny.stars\n",
    "y_test = chi.stars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick check of the predictive power of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "rfc.fit(text_train, y_train)\n",
    "y_pred = rfc.predict(text_test)\n",
    "\n",
    "print metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text wound up not being useful. Yelp's API clips access to user reviews to three per restaurant, AND ~ one sentence per review. And while \"star restaurant\", \"michelin\", \"michelin star\", were (somewhat) predictive in Chicago, those are words unlikely to appear in DC reviews. Still, ever optimistic, I decided to create one column, 'natm', that tallies whether 'newamerican' or 'tasting menu' appeared in category/review text. The thinking being that new american really means cutting edge fine dining, which describes many  of DC's most hyped restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(rfc.feature_importances_,\n",
    "                                   index = text_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance',\n",
    "                                                                        ascending=False).head(10)\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sf_ny['natm'] = sf_ny.category.apply(lambda x: 1 if ('newamerican' in x) or ('tasting menu' in x) else 0)\n",
    "chi['natm'] = chi.category.apply(lambda x: 1 if ('newamerican' in x) or ('tasting menu' in x) else 0)\n",
    "dc['natm'] = dc.category.apply(lambda x: 1 if ('newamerican' in x) or ('tasting menu' in x) else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = sf_ny.loc[:, ['num_reviews', 'price', 'rating', 'natm']]\n",
    "y_train = sf_ny.loc[:, 'stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = chi.loc[:, ['num_reviews', 'price', 'rating', 'natm']]\n",
    "y_test = chi.loc[:, 'stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = StandardScaler().fit_transform(X_train)\n",
    "X_test = StandardScaler().fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = {'class_weight': [None, 'balanced'],\n",
    "              'criterion': ['gini', 'entropy'],\n",
    "              'min_samples_leaf': [1, 2]}\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "gs = GridSearchCV(rfc, param_grid).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print gs.best_params_\n",
    "print gs.score(X_test, y_test)\n",
    "print gs.best_estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = gs.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = {'min_samples_leaf': [1, 2, 3],\n",
    "              'max_features': [.2, .3, .4, .6, .8, None]}\n",
    "\n",
    "gs = GridSearchCV(gbc, param_grid, cv = 5).fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print gs.best_params_\n",
    "print gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = gs.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = {'weights': ['uniform', 'distance'],\n",
    "              'metric': ['braycurtis', 'minkowski']}\n",
    "\n",
    "gs = GridSearchCV(knn, param_grid, cv=5).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print gs.best_params_\n",
    "print gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = gs.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svm = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = {'C': np.logspace(-3,3,7),\n",
    "              'gamma': np.logspace(-3,3,7),\n",
    "              'class_weight': [None, 'balanced']}\n",
    "\n",
    "gs = GridSearchCV(svm, param_grid, cv=5).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print gs.best_params_\n",
    "print gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = gs.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a voting ensembler as added 'regularization'. Only those restaurants that were consistently (across multiple models) identified as Michelin material get the nod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm = SVC(gamma=0.01, probability=True)\n",
    "rfc = RandomForestClassifier(criterion='entropy', min_samples_leaf=2)\n",
    "gbc = GradientBoostingClassifier(max_features=0.4, min_samples_leaf=2)\n",
    "knn = KNeighborsClassifier(metric='braycurtis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier()\n",
    "\n",
    "voter = VotingClassifier(estimators=[ ('svm', svm), ('knn', knn), ('rfc', rfc), ('gbc', gbc)],\n",
    "                              voting='soft').fit(X_train, y_train)\n",
    "\n",
    "print voter.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = voter.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DC Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dc_X = dc.loc[:, ['num_reviews', 'price', 'rating', 'natm']]\n",
    "dc_X = StandardScaler().fit_transform(dc_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dc_pred = voter.predict(dc_X)\n",
    "dc['predictions'] = dc_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dc['none'] = voter.predict_proba(dc_X)[:, 0]\n",
    "dc['one'] = voter.predict_proba(dc_X)[:, 1]\n",
    "dc['two'] = voter.predict_proba(dc_X)[:, 2]\n",
    "dc['three'] = voter.predict_proba(dc_X)[:, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at the cluster of points around the origin. Not the entirety of the cell defined by 0.0 and 0.1 on the 'two' axis and 0.0 and 0.02 on the 'three' axis -- just the bottom left corner of that cell. That cluster represents ~180 restaurants, and among them 9 restaurants that my algorithm predicts. That seems to me very fluky, a coincidence of review count equalling the review count of a Michelin starred restaurant, for instance. So I'm going to drop all predictions within that cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.lmplot('two', 'three', data=dc, hue='predictions', fit_reg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Won't include Off the Record because it is a bar -- not a restaurant.\n",
    "dc.loc[(dc.predictions > 0) & (dc.two + dc.three > 0.03)]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
